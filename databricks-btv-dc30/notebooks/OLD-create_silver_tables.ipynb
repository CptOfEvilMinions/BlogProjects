{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "858bcf4a-f92f-4be7-9d25-0ff672574b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create silver tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39d1db7c-6f78-4293-8161-8e845fb02c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "__CATALOG = \"btv_dc30\"\n",
    "__BRONZE_SCHEMA = \"bronze\"\n",
    "__SILVER_SCHEMA = \"silver\"\n",
    "__BRONZE_BUCKET = \"btv-dc30-bronze-t7rrh\"\n",
    "__SILVER_BUCKET = \"btv-dc30-silver-t7rrh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a7e4374-2616-4ad3-9244-d17e42890268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Sysmon Silver tables\n",
    "\n",
    "This code reads Sysmon event data from a bronze Delta table and organizes it by event ID using a predefined mapping of event IDs to human-readable event names. For each event type, it filters the bronze data to select relevant columns, renames some fields for clarity, and removes the original nested `winlog` column. Then, it writes each filtered DataFrame as a separate Delta table in S3 under a silver schema, named according to the event type. This process effectively splits the raw event data into distinct, structured tables for easier analysis by event category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f777aa4-4785-44c2-b05f-b03c5e3fdf46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "import re\n",
    "\n",
    "\n",
    "sysmon_event_names = {\n",
    "    \"1\": \"ProcessCreation\",\n",
    "    \"2\": \"ProcessChangedAFileCreationTime\",\n",
    "    \"3\": \"NetworkConnection\",\n",
    "    \"4\": \"SysmonServiceStateChanged\",\n",
    "    \"5\": \"ProcessTerminated\",\n",
    "    \"6\": \"DriverLoaded\",\n",
    "    \"7\": \"ImageLoaded\",\n",
    "    \"8\": \"CreateRemoteThread\",\n",
    "    \"9\": \"RawAccessRead\",\n",
    "    \"10\": \"ProcessAccess\",\n",
    "    \"11\": \"FileCreate\",\n",
    "    \"12\": \"RegistryEventObjectCreateAndDelete\",\n",
    "    \"13\": \"RegistryEventValueSet\",\n",
    "    \"14\": \"RegistryEventKeyAndValueRename\",\n",
    "    \"15\": \"FileCreateStreamHash\",\n",
    "    \"16\": \"ServiceConfigurationChange\",\n",
    "    \"17\": \"PipeEventPipeCreated\",\n",
    "    \"18\": \"PipeEventPipeConnected\",\n",
    "    \"19\": \"WmiEventFilterActivityDetected\",\n",
    "    \"20\": \"WmiEventConsumerActivityDetected\",\n",
    "    \"21\": \"WmiEventConsumerToFilterActivityDetected\",\n",
    "    \"22\": \"DnsEventDnsQuery\",\n",
    "    \"23\": \"FileDeleteArchived\",\n",
    "    \"24\": \"ClipboardChange\",\n",
    "    \"25\": \"ProcessTampering\",\n",
    "    \"26\": \"FileDeleteDetected\",\n",
    "    \"27\": \"FileBlockExecutable\",\n",
    "    \"28\": \"FileBlockShredding\",\n",
    "    \"29\": \"FileExecutableDetected\",\n",
    "    \"255\": \"Error\"\n",
    "}\n",
    "\n",
    "\n",
    "# Read from bronze table\n",
    "bronze_df = spark.read.table(f\"{__CATALOG}.{__BRONZE_SCHEMA}.sysmon\") \\\n",
    "    .drop(\"@timestamp\", \"@version\", \"agent\", \"ecs\", \"message\", \"log\", \"tags\", \"host\")\n",
    "    \n",
    "# Event IDs that should include \"file\"\n",
    "file_event_ids = {'2', '7', '11', '15', '17', '6', '18'}\n",
    "registry_event_ids = {'12', '13', '14'}\n",
    "\n",
    "dfs_by_event_name = {}\n",
    "for event_id, event_name in sysmon_event_names.items():\n",
    "    base_cols = [\n",
    "        col(\"winlog\"),\n",
    "        col(\"rule.name\").alias(\"rule\"),\n",
    "        col(\"process\"),\n",
    "        col(\"event\"),\n",
    "    ]\n",
    "    \n",
    "    # Conditionally add \"file\"\n",
    "    if event_id in file_event_ids:\n",
    "        base_cols.append(col(\"file\").alias(\"file\"))\n",
    "\n",
    "    # Conditionally add \"registry\"\n",
    "    if event_id in registry_event_ids:\n",
    "        base_cols.append(col(\"registry\").alias(\"registry\"))\n",
    "    \n",
    "    dfs_by_event_name[event_name] = (\n",
    "        bronze_df.filter(col(\"winlog.event_id\") == event_id).select(*base_cols)\n",
    "    )\n",
    "\n",
    "\n",
    "clean_dfs_by_event_name = {}\n",
    "for table_name, df in dfs_by_event_name.items():\n",
    "    # Get struct fields\n",
    "    columns_field = df.schema[\"winlog\"]\n",
    "    field_names = [f.name for f in columns_field.dataType.fields]\n",
    "\n",
    "    # Find which fields are entirely null\n",
    "    non_null_fields = []\n",
    "    for f in field_names:\n",
    "        non_null_count = df.agg(count(when(col(f\"winlog.{f}\").isNotNull(), f\"winlog.{f}\"))).collect()[0][0]\n",
    "        if non_null_count > 0:  # keep only fields that have at least one non-null\n",
    "            non_null_fields.append(f)\n",
    "\n",
    "    # Select only useful fields + any other winlog you want\n",
    "    clean_df = df.select(\n",
    "        *[col(f\"winlog.{f}\").alias(f) for f in non_null_fields],\n",
    "        col(\"rule\"),\n",
    "        col(\"process\"),\n",
    "        col(\"event\"),\n",
    "    ).drop(\"api\",\"channel\", \"event_id\", \"opcode\", \"provider_name\", \"version\")\n",
    "\n",
    "    clean_dfs_by_event_name[table_name] = clean_df\n",
    "\n",
    "\n",
    "\n",
    "def camel_to_snake(name: str) -> str:\n",
    "    s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "\n",
    "for event_name, df in clean_dfs_by_event_name.items():\n",
    "    table_name = camel_to_snake(event_name)\n",
    "    print (f\"sysmon_{table_name}\")\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .option(\"path\", f\"s3://{__SILVER_BUCKET}/sysmon/{table_name}\") \\\n",
    "        .saveAsTable(f\"{__CATALOG}.{__SILVER_SCHEMA}.sysmon_{table_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa8c020f-20a5-47dc-a2b5-9f59b74e922b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Osquery silver tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f437df42-d92c-4ee4-80e1-8c207852438f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    StructField, \n",
    "    StringType, \n",
    "    LongType, \n",
    "    IntegerType, \n",
    "    BooleanType, \n",
    "    MapType\n",
    ")\n",
    "\n",
    "\n",
    "bronze_df = spark.read.table(f\"{__CATALOG}.{__BRONZE_SCHEMA}.osquery\") \\\n",
    "    .select(\"host\", \"json\")\n",
    "\n",
    "osq_by_table_name = {\n",
    "    row.name: bronze_df.filter(\n",
    "        col(\"json.name\") == row[\"name\"]\n",
    "    ).select(\n",
    "        col(\"json.action\").alias(\"update\"),\n",
    "        col(\"json.hostIdentifier\").alias(\"hostIdentifier\"),\n",
    "        col(\"json.unixTime\").alias(\"unixTime\"),\n",
    "        col(\"json.columns\").alias(\"columns\")\n",
    "    )\n",
    "    for row in bronze_df.select(\"json.name\").distinct().collect()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b791a8f-e368-4a17-9511-2a74372ff538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "import re\n",
    "\n",
    "clean_osq_by_table_name = {}\n",
    "for table_name, df in osq_by_table_name.items():\n",
    "    # Get struct fields\n",
    "    columns_field = df.schema[\"columns\"]\n",
    "    field_names = [f.name for f in columns_field.dataType.fields]\n",
    "\n",
    "    # Find which fields are entirely null\n",
    "    non_null_fields = []\n",
    "    for f in field_names:\n",
    "        non_null_count = df.agg(count(when(col(f\"columns.{f}\").isNotNull(), f\"columns.{f}\"))).collect()[0][0]\n",
    "        if non_null_count > 0:  # keep only fields that have at least one non-null\n",
    "            non_null_fields.append(f)\n",
    "\n",
    "    # Select only useful fields + any other columns you want\n",
    "    clean_df = df.select(\n",
    "        *[col(f\"columns.{f}\").alias(f) for f in non_null_fields],\n",
    "        col(\"update\").alias(\"update\"),\n",
    "        col(\"hostIdentifier\").alias(\"hostIdentifier\"),\n",
    "        col(\"unixTime\").alias(\"unixTime\")\n",
    "    )\n",
    "\n",
    "    clean_osq_by_table_name[table_name] = clean_df\n",
    "\n",
    "\n",
    "\n",
    "for table_name, df in clean_osq_by_table_name.items():\n",
    "    tn= re.sub(r'pack_([a-z]+(_)){2}', '', f\"{table_name.replace('-', '_')}\")\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"path\", f\"s3://{__SILVER_BUCKET}/osquery/{tn}\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{__CATALOG}.{__SILVER_SCHEMA}.osquery_{tn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a563488-f8c6-4797-8ee6-8a738bbdd7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Zeek silver tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08dce0b9-b3ad-4b70-9e0e-76b267fc6b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broker\n",
      "capture_loss\n",
      "cluster\n",
      "conn\n",
      "dce_rpc\n",
      "dns\n",
      "dpd\n",
      "files\n",
      "http\n",
      "kerberos\n",
      "known_certs\n",
      "known_services\n",
      "notice\n",
      "ntlm\n",
      "ntp\n",
      "pe\n",
      "rdp\n",
      "reporter\n",
      "smb_files\n",
      "smb_mapping\n",
      "smtp\n",
      "software\n",
      "ssl\n",
      "stats\n",
      "tunnel\n",
      "weird\n",
      "x509\n"
     ]
    }
   ],
   "source": [
    "tables = spark.catalog.listTables(f\"{__CATALOG}.{__BRONZE_SCHEMA}\")\n",
    "\n",
    "# Filter tables that start with 'osquery_'\n",
    "zeek_bronze_tables = [t.name for t in tables if t.name.startswith(\"zeek\")]\n",
    "\n",
    "for table_name in zeek_bronze_tables:\n",
    "    sourcetype = table_name.split(\"zeek_\")[1]\n",
    "    print(sourcetype)\n",
    "\n",
    "    bronze_df = spark.read.table(f\"{__CATALOG}.{__BRONZE_SCHEMA}.{table_name}\")\n",
    "\n",
    "    bronze_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"path\", f\"s3://{__SILVER_BUCKET}/zeek/{sourcetype}\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(f\"{__CATALOG}.{__SILVER_SCHEMA}.{table_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8ddaf84-3478-4b9c-92f6-3331a60ddaf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Wineventlogs silver tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e1ecf7-e3ae-4e42-8ca6-7e7a2008d42b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- winlog: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "from pyspark.sql.functions import map_keys, explode, col\n",
    "import re\n",
    "\n",
    "\n",
    "# Read from bronze table\n",
    "bronze_wineventlogs_df = spark.read.table(f\"{__CATALOG}.{__BRONZE_SCHEMA}.wineventlogs\") \\\n",
    "    .filter(col(\"winlog.provider_name\") != \"Microsoft-Windows-Sysmon\") \\\n",
    "    .select(\n",
    "        col(\"event\"),\n",
    "        col(\"winlog\"),\n",
    "    )\n",
    "\n",
    "keys_df = bronze_wineventlogs_df.select(explode(map_keys(col(\"winlog\"))).alias(\"key\"))\n",
    "unique_keys_list = [row[\"key\"] for row in keys_df.distinct().collect()]\n",
    "\n",
    "flatten_df = bronze_wineventlogs_df.select(\n",
    "    *[col(f\"winlog.{field}\").alias(field) for field in unique_keys_list],\n",
    "    col(\"event\"),\n",
    ").drop(\"api\",\"channel\", \"event_id\", \"opcode\", \"provider_name\", \"version\")\n",
    "\n",
    "\n",
    "flatten_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .option(\"path\", f\"s3://{__SILVER_BUCKET}/wineventlogs\") \\\n",
    "    .saveAsTable(f\"{__CATALOG}.{__SILVER_SCHEMA}.wineventlogs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3dfeb41-add0-43b7-8090-85b84763375d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_silver_tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
